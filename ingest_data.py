"""
Data Ingestion Script - Create Chroma Vector Database
Uses Cohere FREE embeddings - NO CREDIT CARD NEEDED!
Run this locally BEFORE deploying to Streamlit Cloud
"""

import os
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_cohere import CohereEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv

load_dotenv()

# Configuration
DATA_DIR = "./data"  # Put your healthcare PDFs/TXTs here
CHROMA_DIR = "./chroma_db"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

def load_documents(data_dir):
    """Load documents from directory"""
    print(f"ğŸ“‚ Loading documents from {data_dir}...")

    documents = []

    # Load PDFs
    if os.path.exists(data_dir):
        try:
            pdf_loader = DirectoryLoader(
                data_dir,
                glob="**/*.pdf",
                loader_cls=PyPDFLoader,
                show_progress=True
            )
            pdf_docs = pdf_loader.load()
            documents.extend(pdf_docs)
            print(f"  âœ… Loaded {len(pdf_docs)} PDF documents")
        except Exception as e:
            print(f"  âš ï¸ No PDFs found or error loading: {e}")

        # Load text files
        try:
            txt_loader = DirectoryLoader(
                data_dir,
                glob="**/*.txt",
                loader_cls=TextLoader,
                show_progress=True,
                loader_kwargs={"encoding": "utf-8"}
            )
            txt_docs = txt_loader.load()
            documents.extend(txt_docs)
            print(f"  âœ… Loaded {len(txt_docs)} TXT documents")
        except Exception as e:
            print(f"  âš ï¸ No TXT files found or error loading: {e}")

    print(f"\nâœ… Total: {len(documents)} documents loaded")
    return documents

def split_documents(documents):
    """Split documents into chunks"""
    print("\nâœ‚ï¸ Splitting documents into chunks...")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        add_start_index=True
    )

    chunks = text_splitter.split_documents(documents)
    print(f"âœ… Created {len(chunks)} chunks")

    return chunks

def create_vector_store(chunks):
    """Create and persist Chroma vector store with Cohere embeddings"""
    print("\nğŸ”® Creating vector embeddings with Cohere (FREE!)...")

    # Get Cohere API key
    cohere_api_key = os.getenv("COHERE_API_KEY")
    if not cohere_api_key:
        print("\nâŒ COHERE_API_KEY not found in .env file!")
        print("\nğŸ”‘ Get your FREE Cohere API key:")
        print("1. Go to: https://dashboard.cohere.com/api-keys")
        print("2. Sign up (no credit card needed!)")
        print("3. Copy your API key")
        print("4. Create .env file with: COHERE_API_KEY=your_key_here")
        raise ValueError("Missing COHERE_API_KEY")

    # Initialize Cohere embeddings (FREE tier!)
    print("ğŸ”‘ Initializing Cohere embeddings...")
    embeddings = CohereEmbeddings(
        cohere_api_key=cohere_api_key,
        model="embed-english-light-v3.0"  # FREE model, works great!
    )

    print(f"\nğŸ“Š Processing {len(chunks)} chunks...")
    print("â³ This may take a few minutes for large datasets...")
    print("ğŸ’¡ Tip: Cohere FREE tier = 100 embeds/minute")

    # Create vector store (with progress updates)
    try:
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=CHROMA_DIR
        )

        print(f"\nâœ… Vector store created successfully!")
        print(f"ğŸ“ Location: {CHROMA_DIR}")
        print(f"ğŸ“Š Total vectors: {vectorstore._collection.count()}")

    except Exception as e:
        print(f"\nâŒ Error creating vector store: {e}")
        print("\nğŸ’¡ Common issues:")
        print("- Check your COHERE_API_KEY is valid")
        print("- Ensure you have internet connection")
        print("- Try reducing chunk size if you have many documents")
        raise

    return vectorstore

def main():
    """Main ingestion pipeline"""
    print("="*60)
    print("ğŸš€ HK Healthcare RAG Data Ingestion")
    print("ğŸ’° Using Cohere FREE embeddings - No credit card needed!")
    print("="*60)

    # Check if data directory exists
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
        print(f"\nğŸ“ Created {DATA_DIR} directory.")
        print("\nâš ï¸ Please add your Hong Kong healthcare documents there!")
        print("\nSupported formats: PDF, TXT")
        print("\nExample files to add:")
        print("- HK hospital directories")
        print("- Healthcare service guides")
        print("- Medical information PDFs")
        print("- Clinic information documents")
        print("\nğŸ‘‰ After adding files, run this script again!")
        return

    # Check if directory has files
    files = [f for f in os.listdir(DATA_DIR) if f.endswith(('.pdf', '.txt'))]
    if not files:
        print(f"\nâš ï¸ No PDF or TXT files found in {DATA_DIR}")
        print("\nPlease add documents first!")
        print("\nExample test file you can create:")
        print(f"  echo 'Hong Kong healthcare test document' > {DATA_DIR}/test.txt")
        return

    print(f"\nğŸ“ Found {len(files)} files in {DATA_DIR}:")
    for f in files[:5]:  # Show first 5
        print(f"  - {f}")
    if len(files) > 5:
        print(f"  ... and {len(files)-5} more")

    # Load documents
    documents = load_documents(DATA_DIR)

    if not documents:
        print("\nâŒ No documents loaded. Check your data directory!")
        return

    # Split into chunks
    chunks = split_documents(documents)

    if not chunks:
        print("\nâŒ No chunks created. Check your documents!")
        return

    # Create vector store
    try:
        vectorstore = create_vector_store(chunks)
    except Exception as e:
        print(f"\nâŒ Failed to create vector store: {e}")
        return

    print("\n" + "="*60)
    print("âœ¨ Ingestion Complete! Your vector database is ready.")
    print("="*60)
    print(f"\nğŸ“Š Summary:")
    print(f"  - Documents: {len(documents)}")
    print(f"  - Chunks: {len(chunks)}")
    print(f"  - Vector DB: {CHROMA_DIR}")
    print(f"\nğŸš€ Next steps:")
    print("  1. Test locally: streamlit run streamlit_app.py")
    print("  2. Upload chroma_db/ folder to GitHub")
    print("  3. Deploy to Streamlit Cloud!")
    print(f"\nğŸ’¡ Remember: Your vector database is in {CHROMA_DIR}/")
    print("   Make sure to commit this folder to git!")

if __name__ == "__main__":
    main()
